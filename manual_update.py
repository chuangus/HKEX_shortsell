# -*- coding: utf-8 -*-
"""
Created on Mon Dec 20 15:34:49 2021

@author: angus
"""
import pandas as pd
import datetime
import yfinance as yf
import pandas as pd
import requests
import csv
import multitasking
from time import sleep
from bs4 import BeautifulSoup
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from numpy import nan
####################### scraping HKEX data on shares outstanding
### formula for HKEX scraping
multipliers = {'K':1000, 'M':1000000, 'B':1000000000}
def org_table(tickers, tables, row = 0):
    """Organizes a list of tables into one dataframe in the order of specified tickers.
    Row points to which row to look at in the tables to match with the tickers. Useful for organizing threaded output.
    in:  list of dataframes, list, int
    out: dataframe
    """
    tables8 = pd.DataFrame()
    while len(tickers) > 0:
        y = tickers[0]
        abc = len(tickers)
        abc = list(range(0, abc))
        for number in abc:
            try: 
                tables3 = tables [number]
                x = tables3.iloc[row, 0]
                if x == y:
                    tables8 = pd.concat([tables8,tables3], ignore_index=False, axis = 1)
                    tables = tables[:number]+ tables[number+1:]
                    tickers = tickers[1:]
            except IndexError:
                pass
    return tables8

def string_to_float(string):
    if string[-1].isdigit(): # check if no suffix
        return int(string)
    mult = multipliers[string[-1]] # look up suffix to get multiplier
     # convert number to float, multiply by multiplier, then make int
    return float(string[:-1]) * mult

@multitasking.task
def threaded_gather_data(ticker, output):
    driver = webdriver.Chrome(options=chrome_options) ### use google chrome
    url = 'https://www.hkex.com.hk/Market-Data/Securities-Prices/Equities/Equities-Quote?sym=' + str(ticker) +'&sc_lang=en'
    driver.get(url) ### go to website
    sleep(1) ### gives time for page to load. This is a xueqiu specific solution
    html = driver.page_source ## gather and read HTML       
    driver.quit()
    try:
        soup = BeautifulSoup(html, 'html.parser')
        shr_out = soup.find(class_= "col_issued_shares")
        shr_out = shr_out.get_text()
        shr_out = shr_out.split(' (as at ')
                
    except AttributeError:
        shr_out = ['NA', 'NA']
    try:
        shr_out[1]
    except IndexError:
        shr_out = soup.find_all(class_= "col_issued_shares")
        
        shr_out = str(shr_out)
        shr_out = shr_out.split('<br/>')
        for shares in shr_out:
            if '(Listed' in shares:
                shr_out = shares
        shr_out = shr_out.split(' (Listed ')
    
    mktcap = soup.find(class_= "ico_data col_mktcap") ### mkt cap needs to be fixed for the data scraper and shr_out needs to be fixed for 884 and 6196
    try:
        mktcap = mktcap.get_text()
        mktcap = mktcap.replace('HK$','')
        mktcap = mktcap.replace('HKD','')
        mktcap = mktcap.replace('RMB','')
        mktcap = mktcap.replace(',','')
        mktcap = string_to_float(mktcap)
    except AttributeError:
        mktcap = soup.find(class_= "ico_data col_mktcap tooltip")
        mktcap = mktcap.get_text()
        mktcap = mktcap.replace('H Shares only','')
        mktcap = mktcap.replace('HK$','')
        mktcap = mktcap.replace('HKD','')
        mktcap = mktcap.replace('RMB','')
        mktcap = mktcap.replace(',','')
        mktcap = string_to_float(mktcap)
    # except AttributeError:
    #     mktcap = 'NA'
    abc = [ticker, shr_out[0],shr_out[1] ,mktcap]
    output.append(abc)

### filter the data using a list generated by input
chrome_options = Options()
chrome_options.add_argument('--headless')
chrome_options.add_argument('--no-sandbox')
chrome_options.add_argument('--disable-dev-shm-usage')

# tickers = pd.read_excel('SFC.xlsx')
# tickers2 = tickers.groupby('Stock Code').count()
# tickers2 = tickers2.index    

tickers2 = ["38","107","168","177","317","323","338","347","358","386","390","525","548","564","588","598","670","728","753","763","811","857","874","902","914","921","939","956","991","995","998","1033","1055","1065","1071","1088","1072","1108","1138","1157","1171","1186","1211","1288","1330","1336","1339","1349","1375","1385","1398","1456","1513","1528","1618","1658","1766","1776","1772","1787","1800","1812","1816","1839","1877","1898","1919","1963","1988","2009","2016","2039","2196","2202","2208","2238","2318","2333","2338","2359","2600","2607","2601","2611","2628","2727","2866","2883","2899","3328","3347","3606","3618","3759","3866","3898","3908","3969","3958","3968","3988","3993","3996","6030","6066","6099","6127","6178","6196","6185","6198","6806","6690","6818","6826","6837","6865","6869","6881","6886","9989"]

# def hkex(tickers2):
tables2 = []
for ticker2 in tickers2:
    threaded_gather_data(ticker2, tables2) 
while len(tables2) < len(tickers2): 
    sleep(0.01)
# tables2 = org_table(tickers2, tables2, row = 0) 
    # return tables2
# tables2 = hkex(tickers2)

tables2 = pd.DataFrame(tables2)
tables2.to_excel("HKEX2.xlsx")  







